services:

  # Postgres
  db:
    image: postgres
    container_name: db
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    ports:
      - "5432:5432"
    volumes:
      - db-data:/var/lib/postgresql/data
      - ./db/postgres-init.sql:/docker-entrypoint-initdb.d/postgres-init.sql
    networks:
      - backend-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d Restaurante"]
      interval: 5s
      timeout: 5s
      retries: 5
    
  fill-db:  # Nuevo servicio para tu script
    build: 
      context: .
      dockerfile: Dockerfile.fill-db
    depends_on:
      db:
        condition: service_healthy
    environment:
      DB_HOST: db
    networks:
      - backend-network

  # MongoDB Shard Nodes
  mongors1n1:
    container_name: mongors1n1
    image: mongo
    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017
    ports:
      - 27017:27017
    expose:
      - "27017"
    environment:
      TERM: xterm
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - data1:/data/db
    networks:
      - backend-network

  mongors1n2:
    container_name: mongors1n2
    image: mongo
    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017
    ports:
      - 27027:27017
    expose:
      - "27017"
    environment:
      TERM: xterm
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - data2:/data/db
    networks:
      - backend-network

  mongors1n3:
    container_name: mongors1n3
    image: mongo
    command: mongod --shardsvr --replSet mongors1 --dbpath /data/db --port 27017
    ports:
      - 27037:27017
    expose:
      - "27017"
    environment:
      TERM: xterm
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - data3:/data/db
    networks:
      - backend-network

  # Config Servers (Replica Set de 3 nodos)
  mongo-config1:
    image: mongo
    container_name: mongo-config1
    command: mongod --configsvr --replSet configReplSet --dbpath /data/configdb --port 27017
    networks:
      - backend-network
    volumes:
      - config-data1:/data/configdb

  mongo-config2:
    image: mongo
    container_name: mongo-config2
    command: mongod --configsvr --replSet configReplSet --dbpath /data/configdb --port 27017
    networks:
      - backend-network
    volumes:
      - config-data2:/data/configdb

  mongo-config3:
    image: mongo
    container_name: mongo-config3
    command: mongod --configsvr --replSet configReplSet --dbpath /data/configdb --port 27017
    networks:
      - backend-network
    volumes:
      - config-data3:/data/configdb

  # MongoDB Router
  mongos:
    image: mongo
    container_name: mongos
    command: mongos --configdb configReplSet/mongo-config1:27017,mongo-config2:27017,mongo-config3:27017 --bind_ip_all --port 27017
    ports:
      - "27018:27017"
    depends_on:
      - mongo-config1
      - mongo-config2
      - mongo-config3
      - mongors1n1
      - mongors1n2
      - mongors1n3
    networks:
      - backend-network

  # Redis
  redis:
    image: redis
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    restart: unless-stopped
    networks:
      - backend-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # ElasticSearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.28
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m  # Reduce memoria si es necesario
      - http.host=0.0.0.0  # Aceptar conexiones externas
      - transport.host=0.0.0.0
    ports:
      - "9200:9200"
    volumes:
      - es-data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
    networks:
      - backend-network

  # Base de datos para Keycloak
  keycloak-db:
    image: postgres
    container_name: keycloak-db
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - keycloak-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keycloak -d keycloak"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - backend-network

  # Servicio Keycloak
  keycloak:
    image: quay.io/keycloak/keycloak:24.0.1
    container_name: keycloak
    command: start-dev
    #command: start-dev --import-realm
    environment:
      KC_DB: ${KC_DB}
      KC_DB_URL: ${KC_DB_URL}
      KC_DB_USERNAME: ${KC_DB_USERNAME}
      KC_DB_PASSWORD: ${KC_DB_PASSWORD}
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
    ports:
      - "8080:8080"
    volumes:
      - keycloak-data:/opt/keycloak/data
      #- ./keycloak-realm:/opt/keycloak/data/import
    depends_on:
      keycloak-db:
        condition: service_healthy
    networks:
      - backend-network

  nginx:
    image: nginx:latest
    container_name: nginx
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api1
      - api2
      - search   
    networks:
      - backend-network

  # Servicio API
  api1:
    container_name: api1
    build:
      context: .
      dockerfile: src/api/Dockerfile
    env_file:
      - .env
    environment:
      INSTANCE_NAME: api1
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      DB_NAME: ${DB_NAME}
      DB_HOST: db
      DB_PORT: ${DB_PORT}
      KEYCLOAK_URL: ${KEYCLOAK_URL}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID}
      JWT_SECRET: ${JWT_SECRET}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379
      ELASTICSEARCH_URL: http://elasticsearch:9200
    ports:
      - "5001:5000"
    depends_on:
      elasticsearch:
        condition: service_healthy
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - backend-network

  api2:
    container_name: api2
    build:
      context: .
      dockerfile: src/api/Dockerfile
    env_file:
      - .env
    environment:
      INSTANCE_NAME: api2
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      DB_NAME: ${DB_NAME}
      DB_HOST: db
      DB_PORT: ${DB_PORT}
      KEYCLOAK_URL: ${KEYCLOAK_URL}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID}
      JWT_SECRET: ${JWT_SECRET}
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379
    ports:
      - "5002:5000"
    depends_on:
      - db
      - keycloak
      - redis
      - mongos
    networks:
      - backend-network

  search:
    build:
      context: .
      dockerfile: src/search/Dockerfile
    container_name: search
    env_file:
      - .env
    environment:
      DB_USER: postgres
      DB_PASSWORD: mitzy
      DB_NAME: Restaurante
      DB_HOST: db
      DB_PORT: 5432
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_REALM: reserva-restaurantes
      KEYCLOAK_CLIENT_ID: api-restaurantes
      JWT_SECRET: your_jwt_secret
      REDIS_URL: redis://:${REDIS_PASSWORD}@redis:6379
      ELASTICSEARCH_URL: http://elasticsearch:9200
    ports:
      - "5003:5003"
    networks:
      - backend-network

# Hadoop and Hive
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: hadoop-namenode
    environment:
      CLUSTER_NAME: ${CLUSTER_NAME}
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
    networks:
      - backend-network

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: hadoop-datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - backend-network

  hive-metastore:
    image: postgres:12
    container_name: hive-metastore
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    ports:
      - "5433:5432"
    volumes:
      - metastore_data:/var/lib/postgresql/data
    networks:
      - backend-network

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    environment:
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:8020
      HIVE_METASTORE_DB_TYPE: postgres
      HIVE_METASTORE_HOST: hive-metastore
      HIVE_METASTORE_PORT: 5432
      HIVE_METASTORE_DB_NAME: metastore
      HIVE_METASTORE_DB_USER: hive
      HIVE_METASTORE_DB_PASS: hive
    volumes:
      - ./hive/hive-site.xml:/opt/hive/conf/hive-site.xml
    ports:
      - "10000:10000"
    depends_on:
      - hadoop-namenode
      - hadoop-datanode
      - hive-metastore
    networks:
      - backend-network

  airflow-db:
    image: postgres:15
    container_name: airflow-db
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    networks:
      - backend-network

  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    image: custom-airflow:2.9.1
    container_name: airflow-webserver
    depends_on:
      - airflow-db
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=dpE2CnI5Yrw-g2CxwpKiDorWMj-bfvkiVsnOOetwyVg=
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/dags/data:/opt/airflow/dags/data
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/scripts:/opt/airflow/scripts
    ports:
      - "8081:8080"
    networks:
      - backend-network
    command: webserver


  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    image: custom-airflow:2.9.1
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=dpE2CnI5Yrw-g2CxwpKiDorWMj-bfvkiVsnOOetwyVg=
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/dags/data:/opt/airflow/dags/data
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/scripts:/opt/airflow/scripts
    networks:
      - backend-network
    command: scheduler
   

  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    image: custom-airflow:2.9.1
    container_name: airflow-init
    depends_on:
      - airflow-db
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=dpE2CnI5Yrw-g2CxwpKiDorWMj-bfvkiVsnOOetwyVg=
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/dags/data:/opt/airflow/dags/data
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/scripts:/opt/airflow/scripts
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db upgrade
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
    networks:
      - backend-network

  # Apache Spark (master)
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    command: >
        bash -c "
        mkdir -p /opt/spark/jars &&
        cp /extra-jars/*.jar /opt/spark/jars/ 2>/dev/null || true &&
        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
        --host ${SPARK_MASTER_HOST}
        --port ${SPARK_MASTER_PORT}
        --webui-port ${SPARK_MASTER_WEBUI_PORT}"
    environment:
      - SPARK_MASTER_HOST=${SPARK_MASTER_HOST}
      - SPARK_MASTER_PORT=${SPARK_MASTER_PORT}
      - SPARK_MASTER_WEBUI_PORT=${SPARK_MASTER_WEBUI_PORT}
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    ports:
      - "${SPARK_WEBUI_EXPOSED_PORT:-8082}:${SPARK_MASTER_WEBUI_PORT}"
      - "${SPARK_MASTER_EXPOSED_PORT:-7077}:${SPARK_MASTER_PORT}"
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./jars:/extra-jars
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/dags/data:/opt/airflow/dags/data
    env_file:
      - .env
    networks:
      - backend-network

  # Apache Spark (worker)
  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://${SPARK_MASTER_HOST}:${SPARK_MASTER_PORT}
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./spark-apps:/opt/spark-apps
      - ./jars:/extra-jars
      - ./airflow/scripts:/opt/airflow/scripts
      - ./airflow/dags/data:/opt/airflow/dags/data
    env_file:
      - .env
    networks:
      - backend-network

  # Apache Superset
  superset:
    build:
      context: .
      dockerfile: Dockerfile.superset
    container_name: superset
    ports:
      - "8088:8088"
    env_file:
      - .env
    environment:
      - FLASK_APP=superset.app:create_app()
    volumes:
      - ./superset:/app/superset_home
    networks:
      - backend-network
    command: >
      bash -c "
        superset db upgrade && \
        superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin && \
        superset init && \
        /usr/bin/run-server.sh
      "
  neo4j:
      image: neo4j:5.20.0
      container_name: neo4j
      restart: unless-stopped
      environment:
        NEO4J_AUTH: neo4j/safepassword
        NEO4J_PLUGINS: '["graph-data-science"]'
        NEO4J_apoc_import_file_enabled: "true"
        NEO4J_apoc_import_file_use_neo4j_config: "false"
        NEO4J_dbms_security_procedures_unrestricted: "gds.,apoc."
      ports:
        - "7474:7474"  
        - "7687:7687"  
      volumes:
        - ./data:/var/lib/neo4j/import  
        - neo4j_data:/data
      networks:
        - backend-network
    
  grafos:
      build:
        context: .
        dockerfile: Dockerfile.grafos
      depends_on:
        - neo4j
      volumes:
        - .:/app
      working_dir: /app
      environment:
        - NEO4J_HOST=neo4j
        - NEO4J_PORT=7687
      command: ./wait-for-neo4j.sh
      networks:
        - backend-network


# Configuración de volúmenes
volumes:
  db-data:
  redis-data:
  mongo-data:
  keycloak-db-data:
  keycloak-data:
  data1:
  data2:
  data3:
  config-data1:
  config-data2:
  config-data3:
  es-data:
  metastore_data:
  hadoop_namenode:
  hadoop_datanode:
  airflow-db-data:
  neo4j_data:

# Configuración de red
networks:
  backend-network:
    driver: bridge
